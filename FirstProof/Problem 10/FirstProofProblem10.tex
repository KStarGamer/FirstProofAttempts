\documentclass[11pt]{amsart}

% --- Page layout (expanded margins) ---
\usepackage{geometry}
\geometry{margin=1.30in}

% --- Math packages ---
\usepackage{amsmath,amssymb,amsfonts,amsthm,mathtools,bm}

% --- Misc ---
\usepackage{microtype}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[hidelinks]{hyperref}

% --- Notation/macros ---
\newcommand{\RR}{\mathbb{R}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\vecop}{\operatorname{vec}}
\newcommand{\Tr}{\operatorname{trace}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\kron}{\otimes}
\newcommand{\Had}{\ast}
\newcommand{\ipF}[2]{\left\langle #1,#2\right\rangle_{\mathrm{F}}}
\newcommand{\normF}[1]{\left\lVert #1\right\rVert_{\mathrm{F}}}
\newcommand{\normTwo}[1]{\left\lVert #1\right\rVert_{2}}
\newcommand{\Id}{I}
\newcommand{\diag}{\operatorname{diag}}

% --- Theorem environments ---
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}

% --- Title ---
\title[Matrix-Free PCG for Kernelized CP Completion]{A Matrix-Free Preconditioned Conjugate-Gradient Solver\\
for the Mode-\texorpdfstring{$k$}{k} RKHS Subproblem in Kernelized CP Tensor Completion with Missing Entries}
\author{}
\date{}

\begin{document}
\maketitle

\begin{abstract}
We study the mode-wise alternating least-squares (ALS) update for a kernelized CP tensor completion model with missing (unaligned) data,
focusing on an infinite-dimensional mode constrained to a reproducing kernel Hilbert space (RKHS).
When all CP factors except the mode-$k$ factor are fixed, the RKHS parameterization $A_k=KW$ leads to normal equations of dimension $nr\times nr$,
where $n=n_k$ and $r$ is the CP rank. Forming and solving this system directly costs $\Theta(n^3r^3)$ and is impractical.
We derive a self-adjoint matrix-free operator enabling (preconditioned) conjugate gradients (PCG) using only kernel multiplications and sparse
accumulations over the $q\ll N$ observed tensor entries, with no computation of order $N=\prod_i n_i$ and without forming any Kronecker or Khatri--Rao products.
We rigorously address the positive-(semi)definiteness subtleties arising from a positive semidefinite kernel Gram matrix, and we propose provably SPD preconditioners,
including a robust kernel-block preconditioner and a stronger Kronecker-spectral preconditioner calibrated to the sampling rate.
Per PCG iteration, the matrix-vector product costs $O(qr+n^2r)$ (or $O(nr^2+n^2r)$ with optional preprocessing), and the preconditioner applications
cost $O(n^2r)$ or $O(n^2r+nr^2)$ depending on the choice.
\end{abstract}

\section{Introduction}

Let $\mathcal{T}\in\RR^{n_1\times\cdots\times n_d}$ be a $d$-way tensor with missing (unaligned) entries.
Let $N:=\prod_{i=1}^d n_i$ be the total number of tensor entries and let $q\ll N$ be the number of observed entries.
Kernelized tensor completion methods combine low-rank multilinear structure with smoothness constraints on one or more modes via RKHS penalties.
A standard computational strategy is alternating minimization over the CP factors.
When a mode is constrained to lie in an RKHS, the mode-wise subproblem is typically a large linear system whose naive solution scales cubically
in the total number of unknowns ($nr$), and whose explicit formation is prohibitively expensive for missing data due to the ambient dimension $N$.

This paper gives a self-contained, rigorous derivation of a matrix-free preconditioned conjugate-gradient (PCG) solver for the mode-$k$ subproblem
when mode $k$ is kernelized and data are missing.
Our emphasis is the computation of matrix-vector products and preconditioner applications without forming any object of size $N$ or $M=\prod_{i\neq k} n_i$,
under the regime $n,r<q\ll N$ and $n\ll M$.

\section{Problem setup and the mode-\texorpdfstring{$k$}{k} normal equations}

Fix an index $k\in\{1,\dots,d\}$.
Set
\[
n:=n_k,\qquad M:=\prod_{i\neq k} n_i,\qquad N=nM.
\]
Let the observed index set be
\[
\Omega\subseteq [n_1]\times\cdots\times[n_d],
\qquad |\Omega|=q.
\]
We store observations as pairs $(\bm{i}^{(\ell)},t_\ell)_{\ell=1}^q$ where $\bm{i}^{(\ell)}=(i_1^{(\ell)},\dots,i_d^{(\ell)})\in\Omega$
and $t_\ell=\mathcal{T}_{\bm{i}^{(\ell)}}\in\RR$.

\subsection{Masking operators}

Let $\mathcal{P}_\Omega$ denote the orthogonal projection that keeps entries in $\Omega$ and zeros out all other entries.
On the mode-$k$ unfolding, we write $P_\Omega$ for the corresponding projection acting on $n\times M$ matrices:
\[
(P_\Omega(F))_{i,m}=
\begin{cases}
F_{i,m},&\text{if }(i,m)\text{ corresponds to an observed tensor entry in }\Omega,\\
0,&\text{otherwise}.
\end{cases}
\]
Equivalently, in vectorized form, $P_\Omega$ is an $N\times N$ diagonal projector with $1$'s on observed locations and $0$'s elsewhere.
A common formal device is a selection matrix $S\in\RR^{N\times q}$ such that $S^\top \vecop(F)\in\RR^q$ extracts the observed entries of $F$; then
\[
P_\Omega = SS^\top,\qquad P_\Omega^2=P_\Omega=P_\Omega^\top\succeq 0.
\]
In computations we \emph{never} form $S$ or $P_\Omega$ explicitly; we work directly with the index list $\Omega$.

\subsection{CP factors and Khatri--Rao structure}

Let $r\in\NN$ be the CP rank. For each mode $j\neq k$ we have a factor matrix
\[
A_j\in\RR^{n_j\times r},
\]
which is held fixed during the mode-$k$ update.
Let the Khatri--Rao product over all modes except $k$ be
\[
Z:=A_d\odot\cdots\odot A_{k+1}\odot A_{k-1}\odot\cdots\odot A_1\in\RR^{M\times r},
\]
where $\odot$ denotes the columnwise Kronecker (Khatri--Rao) product.

Let $T\in\RR^{n\times M}$ be the mode-$k$ unfolding of $\mathcal{T}$ in which missing entries are set to $0$.
Define the (masked) MTTKRP
\[
B := TZ\in\RR^{n\times r}.
\]
Because missing entries of $T$ are $0$, $B$ depends only on observed entries and can be assembled from $\Omega$ without forming $T$ or $Z$ (Section~\ref{sec:rhs}).

\subsection{Kernelized mode-\texorpdfstring{$k$}{k} factor}

Let $K\in\RR^{n\times n}$ be a symmetric positive semidefinite (psd) kernel Gram matrix, arising from an RKHS on the mode-$k$ index set.
We parameterize the mode-$k$ factor as
\[
A_k = KW,\qquad W\in\RR^{n\times r}\ \text{unknown}.
\]
Let $w=\vecop(W)\in\RR^{nr}$.

\subsection{Least-squares subproblem and normal equations}

A standard mode-wise subproblem for tensor completion minimizes the squared error on observed entries plus an RKHS ridge penalty.
In the present notation, this becomes
\begin{equation}\label{eq:subproblem}
\min_{W\in\RR^{n\times r}}
\ \frac12 \normTwo{S^\top\vecop(T)-S^\top(Z\kron K)\vecop(W)}^2
+\frac{\lambda}{2}\,\Tr(W^\top K W),
\qquad \lambda>0.
\end{equation}
The Euler--Lagrange equations yield the normal equations
\begin{equation}\label{eq:system}
\Big[(Z\kron K)^\top P_\Omega (Z\kron K) + \lambda (\Id_r\kron K)\Big]\, w
=
(Z\kron K)^\top P_\Omega \vecop(T).
\end{equation}
Since $T$ has zeros on missing entries, $P_\Omega\vecop(T)=\vecop(T)$.
Using the vec--Kronecker identity $(B^\top\kron A)\vecop(X)=\vecop(AXB)$ we have
\[
(Z\kron K)^\top \vecop(T) = (Z^\top\kron K)\vecop(T)=\vecop(KTZ)=\vecop(KB).
\]
Thus the system \eqref{eq:system} can be written in the form used in the problem statement:
\begin{equation}\label{eq:system_B}
\Big[(Z\kron K)^\top P_\Omega (Z\kron K) + \lambda (\Id_r\kron K)\Big]\, w
=
(\Id_r\kron K)\vecop(B).
\end{equation}

\section{Symmetry and positive-(semi)definiteness}\label{sec:spd}

Conjugate gradients (CG) and preconditioned CG (PCG) apply to linear systems with symmetric positive definite (SPD) matrices.
The matrix in \eqref{eq:system_B} is always symmetric and positive semidefinite, but it may be singular if $K$ is singular.
We treat this issue rigorously.

\begin{proposition}[Self-adjointness and psd property]\label{prop:psd}
Let $K\succeq 0$ and $\lambda\ge 0$.
Then the system matrix
\[
A := (Z\kron K)^\top P_\Omega (Z\kron K) + \lambda(\Id_r\kron K)
\]
is symmetric and positive semidefinite. Moreover, for any $W\in\RR^{n\times r}$,
\begin{equation}\label{eq:energy}
\vecop(W)^\top A\,\vecop(W)
=
\normF{P_\Omega(KWZ^\top)}^2 + \lambda\,\Tr(W^\top K W).
\end{equation}
\end{proposition}

\begin{proof}
Symmetry is immediate since $P_\Omega=P_\Omega^\top$ and $K=K^\top$.
For the quadratic form, write $w=\vecop(W)$ and note that
\[
(Z\kron K)w = \vecop(KWZ^\top).
\]
Then, since $P_\Omega$ is an orthogonal projector,
\[
w^\top (Z\kron K)^\top P_\Omega (Z\kron K) w
=
\normTwo{P_\Omega (Z\kron K)w}^2
=
\normF{P_\Omega(KWZ^\top)}^2.
\]
Also $w^\top(\Id_r\kron K)w=\Tr(W^\top K W)$. Summing yields \eqref{eq:energy}.
\end{proof}

\subsection{When is the system SPD?}

\begin{corollary}[SPD condition]\label{cor:spd}
If $K\succ 0$ and $\lambda>0$, then $A\succ 0$ and CG/PCG applies to \eqref{eq:system_B}.
\end{corollary}

\begin{proof}
If $K\succ0$ and $\lambda>0$, then $\Tr(W^\top K W)=\normF{K^{1/2}W}^2>0$ for any $W\neq 0$.
Hence \eqref{eq:energy} implies $w^\top A w>0$ for all $w\neq 0$.
\end{proof}

\subsection{The psd kernel subtlety and two rigorous resolutions}

If $K\succeq 0$ is singular, then $\Tr(W^\top K W)=0$ for any $W$ whose columns lie in $\ker(K)$.
In that case, also $KWZ^\top=0$, hence the data term vanishes, and the full matrix $A$ is singular.
This is not a pathology: it reflects non-identifiability of $W$ under the parameterization $A_k=KW$ when $K$ is singular.

We give two standard, rigorous resolutions.

\begin{assumption}[Either strict PD or a well-posed regularized surrogate]\label{ass:jitter}
Throughout the remainder we assume one of the following:
\begin{enumerate}[label=(\alph*)]
\item \textbf{Strict PD:} $K\succ0$ (e.g.\ for strictly positive definite kernels on distinct points), so Corollary~\ref{cor:spd} applies; or
\item \textbf{Nugget regularization:} we replace $K$ by $\widetilde K := K+\varepsilon \Id_n$ with $\varepsilon>0$, so $\widetilde K\succ0$.
\end{enumerate}
\end{assumption}

\begin{remark}[What does adding $\varepsilon \Id$ change?]\label{rem:jitter}
Replacing $K$ by $\widetilde K=K+\varepsilon\Id$ changes the objective \eqref{eq:subproblem} by replacing the RKHS seminorm term
$\Tr(W^\top K W)$ with $\Tr(W^\top K W)+\varepsilon\normF{W}^2$, i.e.\ adding a small Euclidean ridge on $W$.
This is a standard ``nugget'' or ``jitter'' that makes the normal equations strictly SPD and improves numerical stability.
As $\varepsilon\downarrow 0$, solutions of the regularized problem converge (under mild conditions) to minimum-$\normF{\cdot}$ solutions
among minimizers of the original psd problem; we do not pursue this limit analysis here.
If one insists on \emph{not} modifying the objective, one may instead reparameterize using a factorization of $K$ and solve an SPD system
in the range space; see Remark~\ref{rem:range_space}.
\end{remark}

\begin{remark}[Range-space formulation without modifying the objective]\label{rem:range_space}
Let $K=Q\Lambda Q^\top$ be an eigen-decomposition with $\Lambda=\diag(\lambda_1,\dots,\lambda_n)$, $\lambda_i\ge 0$.
Let $Q_+\in\RR^{n\times m}$ and $\Lambda_+\in\RR^{m\times m}$ collect the positive eigenpairs, where $m=\rank(K)$,
and define $L:=Q_+\Lambda_+^{1/2}\in\RR^{n\times m}$ so that $K=LL^\top$.
Write $A_k=KW=LL^\top W = L U$ with $U:=L^\top W\in\RR^{m\times r}$.
Then $\Tr(W^\top K W)=\normF{U}^2$, and $(Z\kron K)\vecop(W)=(Z\kron L)\vecop(U)$.
The subproblem \eqref{eq:subproblem} is equivalent to
\[
\min_{U\in\RR^{m\times r}}
\ \frac12 \normTwo{S^\top\vecop(T)-S^\top(Z\kron L)\vecop(U)}^2
+\frac{\lambda}{2}\,\normF{U}^2,
\]
whose normal equations are SPD due to the $\lambda\Id$ term (dimension $mr\times mr$).
All matrix-free ideas below carry over verbatim with $K$ replaced by $L$ and with one fewer kernel multiplication per matvec.
This formulation avoids the nugget modification but requires access to (possibly low-rank) factors of $K$.
\end{remark}

For concreteness, we proceed with $\widetilde K\succ0$ (either because $K\succ0$ or because a nugget was added) and drop the tilde
from the notation when no confusion arises.

\section{Kronecker/vec identities and a matrix-free operator}\label{sec:operator}

We derive a matrix-free operator that applies the $nr\times nr$ system matrix to a vector without forming any Kronecker products.

\begin{lemma}[Vec--Kronecker identities]\label{lem:vec_kron}
Let $A\in\RR^{n\times n}$, $B\in\RR^{m\times m}$, and $X\in\RR^{n\times m}$.
Then
\[
(B^\top\kron A)\,\vecop(X) = \vecop(AXB).
\]
In particular, if $Z\in\RR^{M\times r}$ and $K\in\RR^{n\times n}$,
\[
(Z\kron K)\vecop(X)=\vecop(KXZ^\top),
\qquad
(Z\kron K)^\top\vecop(Y)=\vecop(KYZ)
\]
for all conforming matrices $X\in\RR^{n\times r}$ and $Y\in\RR^{n\times M}$.
\end{lemma}

\begin{proof}
Standard; see, e.g., any linear algebra text covering Kronecker products. It follows from bilinearity and checking on basis matrices.
\end{proof}

\begin{proposition}[Matrix-form linear operator]\label{prop:Aop}
Assume $K\succ0$ (or $K$ replaced by $\widetilde K\succ0$).
Define the linear map $\mathcal{A}:\RR^{n\times r}\to\RR^{n\times r}$ by
\begin{equation}\label{eq:Aop}
\boxed{
\mathcal{A}(X)
=
K\Big(P_\Omega(KXZ^\top)\,Z\Big) + \lambda KX.
}
\end{equation}
Then for all $X\in\RR^{n\times r}$,
\[
\vecop(\mathcal{A}(X)) =
\Big[(Z\kron K)^\top P_\Omega (Z\kron K) + \lambda(\Id_r\kron K)\Big]\vecop(X).
\]
Moreover, $\mathcal{A}$ is self-adjoint and SPD with respect to the Frobenius inner product:
\[
\ipF{X}{\mathcal{A}(X)} = \normF{P_\Omega(KXZ^\top)}^2 + \lambda\,\normF{K^{1/2}X}^2 >0 \quad \text{for } X\neq 0.
\]
\end{proposition}

\begin{proof}
Let $x=\vecop(X)$. By Lemma~\ref{lem:vec_kron}, $(Z\kron K)x=\vecop(KXZ^\top)$.
Apply $P_\Omega$ in vectorized form, then apply $(Z\kron K)^\top$ using Lemma~\ref{lem:vec_kron} again:
\[
(Z\kron K)^\top P_\Omega (Z\kron K)x
=
(Z^\top\kron K)\,\vecop(P_\Omega(KXZ^\top))
=
\vecop(K(P_\Omega(KXZ^\top))Z),
\]
which is the first term in \eqref{eq:Aop}.
The ridge term satisfies $(\Id_r\kron K)\vecop(X)=\vecop(KX)$, giving the second term.

Self-adjointness and SPD follow from Proposition~\ref{prop:psd} and Corollary~\ref{cor:spd}, transported from vector to matrix form via $\vecop$.
\end{proof}

\section{Observed-index representation and no \texorpdfstring{$N$}{N}/\texorpdfstring{$M$}{M} objects}

The operator \eqref{eq:Aop} appears to involve $KXZ^\top\in\RR^{n\times M}$ and the projector $P_\Omega$ acting on an $n\times M$ matrix.
We now show that both can be applied using only the $q$ observed entries without forming any length-$N$ vector, any $n\times M$ matrix, or $Z$ itself.

\subsection{Evaluating rows of the Khatri--Rao product on demand}

For each observation $\ell\in\{1,\dots,q\}$ with tensor index $\bm{i}^{(\ell)}=(i_1^{(\ell)},\dots,i_d^{(\ell)})$,
define the mode-$k$ row index
\[
i^{(\ell)}:=i_k^{(\ell)}\in\{1,\dots,n\}.
\]
The unfolding column index $m^{(\ell)}\in\{1,\dots,M\}$ corresponding to $(i_1^{(\ell)},\dots,i_{k-1}^{(\ell)},i_{k+1}^{(\ell)},\dots,i_d^{(\ell)})$
need not be formed.
Instead we compute the corresponding \emph{row of $Z$} directly from the fixed factor matrices:
\begin{equation}\label{eq:zell}
z^{(\ell)}
:=
A_d[i_d^{(\ell)},:] \odot \cdots \odot A_{k+1}[i_{k+1}^{(\ell)},:]
\odot
A_{k-1}[i_{k-1}^{(\ell)},:] \odot \cdots \odot A_1[i_1^{(\ell)},:]
\in\RR^r.
\end{equation}
Computing $z^{(\ell)}$ costs $O((d-1)r)$ flops.

\begin{remark}[Precompute vs.\ on-the-fly]\label{rem:precompute_z}
Since the $A_j$ ($j\neq k$) are fixed during the mode-$k$ solve, one can precompute and store all $z^{(\ell)}$ once per outer ALS step:
time $O(q(d-1)r)$ and memory $O(qr)$.
If memory is constrained, one may recompute $z^{(\ell)}$ on-the-fly inside each PCG iteration, increasing the per-iteration cost of the masked step
from $O(qr)$ to $O(q(d-1)r)$.
\end{remark}

\section{Matrix-free matrix-vector products}\label{sec:matvec}

We now derive an explicit algorithm for applying $\mathcal{A}$ in \eqref{eq:Aop}
to a matrix $X\in\RR^{n\times r}$ using only kernel multiplications and a sparse accumulation over $\Omega$.

\subsection{Derivation}

Let
\[
G := KX \in \RR^{n\times r}.
\]
The matrix $KXZ^\top$ is $GZ^\top\in\RR^{n\times M}$.
For observation $\ell$, the entry of $GZ^\top$ at the observed unfolding coordinate $(i^{(\ell)},m^{(\ell)})$ equals
\[
(GZ^\top)_{i^{(\ell)},m^{(\ell)}}
=
G[i^{(\ell)},:]\cdot z^{(\ell)},
\]
where $G[i,:]\in\RR^{1\times r}$ denotes row $i$ and $\cdot$ is the Euclidean inner product.
Thus, defining
\[
u_\ell := G[i^{(\ell)},:]\cdot z^{(\ell)} \in \RR,
\]
the masked matrix $P_\Omega(GZ^\top)$ is zero everywhere except at the $q$ observed positions, where it equals $u_\ell$.

We need to form
\[
H := P_\Omega(GZ^\top)\,Z\in\RR^{n\times r}.
\]
Since only observed entries contribute, $H$ can be computed by \emph{scatter-add} over observations:
\begin{equation}\label{eq:H_scatter}
H[i,:] = \sum_{\ell:\,i^{(\ell)}=i} u_\ell\, z^{(\ell)\top},
\qquad i=1,\dots,n.
\end{equation}
Finally,
\[
\mathcal{A}(X) = K H + \lambda G.
\]

\subsection{Algorithm and complexity}

\begin{algorithm}[t]
\caption{Matrix-free matvec $Y=\mathcal{A}(X)$}\label{alg:matvec}
\begin{algorithmic}[1]
\Require $X\in\RR^{n\times r}$, kernel matrix $K\in\RR^{n\times n}$, ridge $\lambda>0$,
observations $\{i^{(\ell)}, z^{(\ell)}\}_{\ell=1}^q$.
\Ensure $Y=\mathcal{A}(X)\in\RR^{n\times r}$.
\State $G \gets KX$ \Comment{$O(n^2r)$ for dense $K$}
\State $H \gets 0\in\RR^{n\times r}$
\For{$\ell=1,\dots,q$}
  \State $u \gets G[i^{(\ell)},:]\cdot z^{(\ell)}$ \Comment{$O(r)$}
  \State $H[i^{(\ell)},:] \gets H[i^{(\ell)},:] + u\, z^{(\ell)\top}$ \Comment{$O(r)$}
\EndFor
\State $Y \gets K H + \lambda G$ \Comment{$O(n^2r)$ for dense $K$}
\State \Return $Y$
\end{algorithmic}
\end{algorithm}

\begin{proposition}[Matvec cost; no $N$ or $M$]\label{prop:matvec_cost}
Assume dense $K$.
Given precomputed $\{z^{(\ell)}\}_{\ell=1}^q$, Algorithm~\ref{alg:matvec} computes $Y=\mathcal{A}(X)$ with cost
\[
O(n^2r) + O(qr) + O(n^2r) = O(qr+n^2r).
\]
No object of size $N=nM$ is formed, and neither $Z$ nor any Kronecker product is formed.
If $z^{(\ell)}$ are computed on-the-fly, the masked step costs $O(q(d-1)r)$ instead of $O(qr)$.
\end{proposition}

\begin{proof}
Immediate from the loop counts and two dense kernel multiplications. The absence of $N$- or $M$-sized objects follows by inspection.
\end{proof}

\subsection{Optional preprocessing: row-wise Gram compression}

When $q$ is extremely large and $r$ is small, it can be beneficial to preprocess the observation-dependent Gram matrices
\[
C_i := \sum_{\ell:\,i^{(\ell)}=i} z^{(\ell)}z^{(\ell)\top}\in\RR^{r\times r},\qquad i=1,\dots,n.
\]
This costs $O(qr^2)$ per outer ALS step and stores $O(nr^2)$ numbers.

\begin{proposition}[Row-wise Gram acceleration]\label{prop:Ci}
With $C_i$ precomputed, the masked accumulation in \eqref{eq:H_scatter} satisfies
\[
H[i,:] = G[i,:]\,C_i\qquad (i=1,\dots,n),
\]
so the masked step can be computed in $O(nr^2)$ per matvec (instead of $O(qr)$).
Thus the matvec cost becomes $O(n^2r+nr^2)$ per iteration.
\end{proposition}

\begin{proof}
For fixed $i$, substitute $u_\ell=G[i,:]\cdot z^{(\ell)}$ into \eqref{eq:H_scatter} and factor out $G[i,:]$:
\[
H[i,:]=\sum_{\ell:i^{(\ell)}=i} (G[i,:]z^{(\ell)})\, z^{(\ell)\top}
= G[i,:]\sum_{\ell:i^{(\ell)}=i} z^{(\ell)}z^{(\ell)\top}
= G[i,:]C_i.
\]
\end{proof}

\section{Right-hand side assembly}\label{sec:rhs}

The right-hand side of \eqref{eq:system_B} is $\vecop(KB)$ where $B=TZ$.
Since $T$ has zeros at missing entries, $B$ can be assembled from the observations without forming $T$ or $Z$.

\begin{proposition}[MTTKRP from observed entries]\label{prop:B}
Let $B=TZ$ with $T$ the zero-filled mode-$k$ unfolding. Then
\begin{equation}\label{eq:B_from_obs}
B[i,:]=\sum_{\ell:\,i^{(\ell)}=i} t_\ell\, z^{(\ell)\top},
\qquad i=1,\dots,n.
\end{equation}
Thus $B$ can be computed in $O(qr)$ flops given $\{z^{(\ell)}\}$, with no $N$- or $M$-sized objects.
\end{proposition}

\begin{proof}
By definition, $(TZ)[i,:]=\sum_{m=1}^M T_{i,m} Z[m,:]$.
Only observed entries contribute because $T_{i,m}=0$ if $(i,m)\notin\Omega$ in unfolding coordinates.
Each observed entry in row $i$ contributes $t_\ell z^{(\ell)\top}$, yielding \eqref{eq:B_from_obs}.
\end{proof}

After assembling $B$, compute $KB$ in $O(n^2r)$ for dense $K$, and set $b=\vecop(KB)$.

\section{PCG in matrix form}

We solve the SPD linear system
\begin{equation}\label{eq:matrix_system}
\mathcal{A}(W) = KB,
\qquad W\in\RR^{n\times r},
\end{equation}
where $\mathcal{A}$ is given by \eqref{eq:Aop}.
Because $\vecop$ is an isometry between $(\RR^{n\times r},\ipF{\cdot}{\cdot})$ and $(\RR^{nr},\langle\cdot,\cdot\rangle)$,
running CG/PCG on \eqref{eq:system_B} is equivalent to running CG/PCG on \eqref{eq:matrix_system} using Frobenius inner products.

Let $M:\RR^{n\times r}\to\RR^{n\times r}$ be an SPD preconditioner (linear, self-adjoint and SPD w.r.t.\ $\ipF{\cdot}{\cdot}$).
PCG requires:
\begin{enumerate}[label=(\roman*)]
\item one application of $\mathcal{A}$ per iteration (Algorithm~\ref{alg:matvec}), and
\item one application of $M^{-1}$ per iteration (Section~\ref{sec:precond}).
\end{enumerate}

\begin{algorithm}[t]
\caption{PCG for $\mathcal{A}(W)=KB$ in matrix form}\label{alg:pcg}
\begin{algorithmic}[1]
\Require Initial guess $W_0\in\RR^{n\times r}$, tolerance $\tau>0$, operator $\mathcal{A}$, RHS $KB$, SPD preconditioner $M$.
\Ensure Approximate solution $W$.
\State $R_0 \gets KB - \mathcal{A}(W_0)$
\State $U_0 \gets M^{-1}(R_0)$
\State $D_0 \gets U_0$
\For{$j=0,1,2,\dots$}
  \State $Q_j \gets \mathcal{A}(D_j)$
  \State $\alpha_j \gets \ipF{R_j}{U_j}/\ipF{D_j}{Q_j}$
  \State $W_{j+1} \gets W_j + \alpha_j D_j$
  \State $R_{j+1} \gets R_j - \alpha_j Q_j$
  \If{$\normF{R_{j+1}}/\normF{KB} \le \tau$}
    \State \Return $W_{j+1}$
  \EndIf
  \State $U_{j+1} \gets M^{-1}(R_{j+1})$
  \State $\beta_j \gets \ipF{R_{j+1}}{U_{j+1}}/\ipF{R_j}{U_j}$
  \State $D_{j+1} \gets U_{j+1} + \beta_j D_j$
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{remark}[Stopping criteria]
Since $\mathcal{A}$ is SPD, the residual norm $\normF{R_j}$ is a standard and reliable stopping criterion.
One may also monitor the (preconditioned) residual $\normF{U_j}$ or the energy norm.
\end{remark}

\section{Preconditioner design and application costs}\label{sec:precond}

A good preconditioner $M$ should (i) be SPD, (ii) be cheap to apply, and (iii) reduce the condition number of $M^{-1}\mathcal{A}$
to decrease PCG iterations.

We present two principled choices.

\subsection{Kernel-block (ridge-term) preconditioner}

The simplest robust choice is to precondition by the ridge term:
\begin{equation}\label{eq:precond_A}
M_A := \lambda(\Id_r\kron K)\qquad\Longleftrightarrow\qquad M_A(R)=\lambda K R.
\end{equation}
It is SPD since $K\succ0$ and $\lambda>0$.

Applying $M_A^{-1}$ amounts to solving $\lambda K U = R$ column-wise.
With a Cholesky factorization $K=LL^\top$, one application costs $O(n^2r)$ via $r$ triangular solves; the setup costs $O(n^3)$.

\begin{remark}[Approximate solves]
If $n$ is large, one may replace exact Cholesky solves by (incomplete) Cholesky, Nystr\"om approximations, or iterative solves for $KU=R$.
Such approximations can still yield effective preconditioning; the present paper focuses on exact SPD preconditioners for clarity.
\end{remark}

\subsection{Kronecker-spectral preconditioner}

The masked data term $(Z\kron K)^\top P_\Omega (Z\kron K)$ destroys exact Kronecker structure,
but a strong preconditioner can be obtained by approximating it by a Kronecker product motivated by full observation.

\subsubsection{Motivation by full observation and sampling rate}

If all entries were observed, $P_\Omega=\Id_N$, then
\[
(Z\kron K)^\top(Z\kron K) = (Z^\top Z)\kron(K^2).
\]
Under approximately uniform missingness, it is natural to scale this by the sampling rate
\[
\rho := \frac{q}{N}\in(0,1],
\]
which is computable from the tensor dimensions (no $O(N)$ operations are required; $N$ is used only as a scalar).
This leads to the SPD preconditioner
\begin{equation}\label{eq:precond_B}
M_B := \rho\,(G_Z\kron K^2) + \lambda(\Id_r\kron K),
\qquad G_Z\approx Z^\top Z.
\end{equation}
Since $K\succ0$ and $\lambda>0$, $M_B$ is SPD for any symmetric $G_Z\succeq 0$ and any $\rho\ge 0$.

\begin{remark}[Choice of $G_Z$]
Two practical options avoid forming $Z$:
\begin{enumerate}[label=(\roman*)]
\item \textbf{Exact Khatri--Rao Gram:} $G_Z=Z^\top Z$ can be computed using the Hadamard identity
\[
Z^\top Z = \Had_{j\neq k} (A_j^\top A_j),
\]
which costs $O(\sum_{j\neq k} n_j r^2)$; see Lemma~\ref{lem:kr_gram}.
\item \textbf{Observed Gram:} $G_{Z,\Omega}:=\sum_{\ell=1}^q z^{(\ell)}z^{(\ell)\top}$, costing $O(qr^2)$,
which can better reflect nonuniform sampling.
\end{enumerate}
\end{remark}

\begin{lemma}[Khatri--Rao Gram identity]\label{lem:kr_gram}
Let $Z=A_s\odot\cdots\odot A_1$ be a Khatri--Rao product of conforming matrices.
Then
\[
Z^\top Z = (A_s^\top A_s)\Had\cdots\Had(A_1^\top A_1).
\]
\end{lemma}

\begin{proof}
The $(p,q)$ entry of $Z^\top Z$ is the inner product between the $p$-th and $q$-th columns of $Z$.
Each column of $Z$ is the Kronecker product of the corresponding columns of the $A_j$.
Inner products of Kronecker products factor as products of inner products, yielding the Hadamard product identity.
\end{proof}

\subsubsection{Fast application of $M_B^{-1}$}

Let $G_Z=U\Sigma U^\top$ with $\Sigma=\diag(\sigma_1,\dots,\sigma_r)\succeq 0$,
and $K=Q\Lambda Q^\top$ with $\Lambda=\diag(\kappa_1,\dots,\kappa_n)\succ 0$.
Then
\[
M_B = (U\kron Q)\ \diag\big(\rho\,\sigma_j\,\kappa_p^2 + \lambda\,\kappa_p\big)_{p=1,\dots,n;\ j=1,\dots,r}\ (U\kron Q)^\top.
\]
Thus, for $R\in\RR^{n\times r}$, $U=M_B^{-1}(R)$ can be computed as follows:
\[
\widehat R := Q^\top R\,U \in \RR^{n\times r},\qquad
\widehat U_{p,j} := \frac{\widehat R_{p,j}}{\rho\,\sigma_j\,\kappa_p^2 + \lambda\,\kappa_p},
\qquad
U := Q\,\widehat U\,U^\top.
\]
The per-application cost is $O(n^2r+nr^2)$ for dense orthogonal transforms, after one-time eigendecompositions:
$O(n^3+r^3)$.

\begin{remark}[When is $M_B$ worth it?]
$M_B$ can significantly reduce PCG iterations when the masked term dominates and when $K$ and $G_Z$ have favorable spectra.
It is most attractive when $r$ is modest and an eigendecomposition of $K$ is acceptable (or already available from kernel methods).
If one prefers only Cholesky factors, $M_A$ is simpler and robust.
\end{remark}

\section{Overall complexity and memory (no $O(N)$ terms)}\label{sec:complexity}

We summarize costs for one mode-$k$ update, assuming $K$ is dense.
Let $t$ be the PCG iteration count.

\subsection{Precomputations per outer ALS step}

\begin{itemize}[leftmargin=2.0em]
\item Compute $z^{(\ell)}$ for $\ell=1,\dots,q$ (optional but typical): $O(q(d-1)r)$ time, $O(qr)$ memory.
\item Assemble $B$ via \eqref{eq:B_from_obs}: $O(qr)$ time, $O(nr)$ memory.
\item Form $KB$: $O(n^2r)$ time.
\item Optional: precompute $C_i$ (row-wise Grams): $O(qr^2)$ time, $O(nr^2)$ memory.
\item Preconditioner setup:
\begin{itemize}[leftmargin=1.5em]
\item $M_A$: Cholesky of $K$: $O(n^3)$ time, $O(n^2)$ memory.
\item $M_B$: eigendecompositions of $K$ and $G_Z$: $O(n^3+r^3)$ time, $O(n^2+r^2)$ memory
(plus computing $G_Z$ or $G_{Z,\Omega}$ as discussed).
\end{itemize}
\end{itemize}

\subsection{Per PCG iteration}

Each iteration performs one matvec and one preconditioner application, plus $O(nr)$ inner products and saxpy operations.

\begin{itemize}[leftmargin=2.0em]
\item Matvec $\mathcal{A}(X)$:
\begin{itemize}[leftmargin=1.5em]
\item without $C_i$: $O(qr+n^2r)$ time;
\item with $C_i$: $O(nr^2+n^2r)$ time.
\end{itemize}
\item Preconditioner apply:
\begin{itemize}[leftmargin=1.5em]
\item $M_A^{-1}$: $O(n^2r)$ time;
\item $M_B^{-1}$: $O(n^2r+nr^2)$ time.
\end{itemize}
\end{itemize}

Thus the dominant total complexity is
\[
O\Big(
q(d-1)r\ +\ qr\ +\ n^2r\ +\ \mathrm{setup}(M)\ +\ t\cdot(\mathrm{matvec}+\mathrm{apply}(M^{-1}))
\Big),
\]
with \emph{no} computations of order $N=nM$ and no explicit construction of $Z$ or any Kronecker product.

\section{Conclusion}

The mode-$k$ RKHS subproblem in kernelized CP tensor completion with missing entries leads to a large $nr\times nr$ SPD (or psd) linear system.
We derived a rigorously justified matrix-free operator and an observed-index implementation of its matrix-vector products that avoids any $O(N)$ operations.
This enables (preconditioned) conjugate gradients with per-iteration cost $O(qr+n^2r)$, substantially improving over dense $O(n^3r^3)$ solvers.
We addressed the kernel psd subtlety by giving two mathematically sound routes to SPD:
either a nugget-regularized kernel (common in practice) or an exact range-space reformulation.
Finally, we proposed practical SPD preconditioners and detailed their application costs and trade-offs.

\begin{thebibliography}{99}

\bibitem{GolubVanLoan}
G.~H. Golub and C.~F. Van Loan,
\emph{Matrix Computations}, 4th ed.,
Johns Hopkins University Press, 2013.

\bibitem{Saad}
Y.~Saad,
\emph{Iterative Methods for Sparse Linear Systems}, 2nd ed.,
SIAM, 2003.

\bibitem{KoldaBader}
T.~G. Kolda and B.~W. Bader,
Tensor decompositions and applications,
\emph{SIAM Review} \textbf{51} (2009), no.~3, 455--500.

\bibitem{RasmussenWilliams}
C.~E. Rasmussen and C.~K.~I. Williams,
\emph{Gaussian Processes for Machine Learning},
MIT Press, 2006.

\end{thebibliography}

\end{document}
